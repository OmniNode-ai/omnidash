# Configuration file for Omnidash
# Copy this file to .env and configure based on your deployment needs

# =====================================================
# POSTGRESQL DATABASE CONFIGURATION (REQUIRED)
# =====================================================
# Omnidash requires a PostgreSQL database for agent observability and intelligence tracking.
# This connects to the omninode-bridge PostgreSQL instance on 192.168.86.200:5436.
#
# Database: omninode_bridge
# Tables: 30+ tables for agent routing, workflow execution, pattern learning, LLM calls, etc.
# See: shared/intelligence-schema.ts for complete schema
#
# Connection Method 1: Individual variables (used by server/storage.ts)
POSTGRES_HOST=192.168.86.200
POSTGRES_PORT=5436
POSTGRES_DATABASE=omninode_bridge
POSTGRES_USER=postgres
POSTGRES_PASSWORD=YOUR_DATABASE_PASSWORD_HERE

# Connection Method 2: Connection string (takes priority if set)
# DATABASE_URL=postgresql://postgres:YOUR_DATABASE_PASSWORD_HERE@192.168.86.200:5436/omninode_bridge

# =====================================================
# LOCAL HOSTING CONFIGURATION
# =====================================================
HOST=localhost
INTELLIGENCE_SERVICE_PORT=8053
INTELLIGENCE_SERVICE_URL=http://localhost:8053

# =====================================================
# LOGGING & DEBUGGING (OPTIONAL)
# =====================================================
# Set log level for debugging
LOGFIRE_TOKEN=
LOG_LEVEL=INFO

# AI Model API Keys
# Google Gemini API key for cloud-based inference (copied from omnibase_3)
GOOGLE_API_KEY=YOUR_GOOGLE_API_KEY_HERE
GEMINI_API_KEY=YOUR_GEMINI_API_KEY_HERE

# Z.AI API key for GLM-4.6 inference (add your key here after subscribing)
Z_AI_API_KEY=YOUR_Z_AI_API_KEY_HERE
Z_AI_API_URL=https://api.z.ai/api/coding/paas/v4

# Embedding Configuration
# Dimensions for embedding vectors (768 for nomic-embed-text, 1024 for mxbai-embed-large)
# Set to 768 since nomic-embed-text is recommended for Ollama
EMBEDDING_DIMENSIONS=768

# Service authentication token for internal API calls
SERVICE_AUTH_TOKEN=YOUR_SERVICE_AUTH_TOKEN_HERE

# OpenAI API Key for embeddings (currently using dummy - switch to Ollama or add real key)
OPENAI_API_KEY=sk-dummy-key-replace-with-real-or-use-ollama

# =====================================================
# AGENT REGISTRY CONFIGURATION
# =====================================================
# Path to agent definitions directory (omniclaude repository)
# If not set, defaults to ../omniclaude/agents/definitions (sibling directory)
# AGENT_DEFINITIONS_PATH=/path/to/omniclaude/agents/definitions

# =====================================================
# DEMO MODE (default: false)
# =====================================================
# When true, starts mock data generators on the server:
# - Fake heartbeats and registry events via WebSocket
# - Fake Kafka event chains persisted to the event bus store
# No fake data is injected unless this is explicitly set to true.
DEMO_MODE=false

# =====================================================
# CLIENT MOCK DATA (default: false)
# =====================================================
# When true, client-side data sources return mock data instead of calling APIs.
# Useful for offline development or demos without a running backend.
VITE_USE_MOCK_DATA=false

# =====================================================
# VALKEY CACHE SECURITY CONFIGURATION
# =====================================================
# Password for Valkey (Redis fork) distributed cache
# IMPORTANT: Change this in production deployments!
VALKEY_PASSWORD=YOUR_VALKEY_PASSWORD_HERE
VALKEY_URL=redis://:YOUR_VALKEY_PASSWORD_HERE@archon-valkey:6379/0
GH_PAT=YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE

# =====================================================
# EXTERNAL MCP GATEWAY CONFIGURATION
# =====================================================
# Enable external MCP service integration (zen, codanna, serena, etc.)
ARCHON_ENABLE_EXTERNAL_GATEWAY=true

# =====================================================
# KAFKA/REDPANDA CONFIGURATION
# =====================================================
# Remote Redpanda broker on 192.168.86.200
#
# Port Selection:
#   - Port 29092: External published port for host applications (RECOMMENDED for omnidash)
#   - Port 9092: Internal Docker network port (for containerized services only)
#
# Omnidash runs as a host process (not in Docker), so it uses port 29092.
# The server validates broker reachability at startup and logs connection status.

KAFKA_BROKERS=192.168.86.200:29092

# Legacy/compatibility (use KAFKA_BROKERS instead)
KAFKA_BOOTSTRAP_SERVERS=192.168.86.200:29092

# Kafka topics
KAFKA_DOC_TOPIC=dev.omniclaude.docs.evt.documentation-changed.v1

# Feature flags
ENABLE_EVENT_INTELLIGENCE=true
ENABLE_KAFKA_LOGGING=true
ENABLE_REAL_TIME_EVENTS=true

# Performance tuning (milliseconds)
KAFKA_REQUEST_TIMEOUT_MS=5000
KAFKA_PATTERN_DISCOVERY_TIMEOUT_MS=5000
KAFKA_CODE_ANALYSIS_TIMEOUT_MS=10000
KAFKA_QUALITY_ASSESSMENT_TIMEOUT_MS=10000

# =====================================================
# SERVER PORT CONFIGURATION
# =====================================================
# Port for the omnidash Express server (development and production)
PORT=3000

# =====================================================
# KAFKA/REDPANDA ADVANCED CONFIGURATION
# =====================================================
# Consumer Group Configuration
# Consumer group ID for Kafka consumer (tracks offset per consumer group)
# Change this to force reading from beginning (new consumer group)
KAFKA_CONSUMER_GROUP_ID=omnidash-consumers-v2

# Client ID for Kafka connections (used for logging and monitoring)
KAFKA_CLIENT_ID=omnidash-event-consumer

# Health Check Client ID (separate client for health checks)
KAFKA_HEALTH_CHECK_CLIENT_ID=omnidash-health-check

# Connection Timeouts (milliseconds)
# Connection timeout for initial broker connection
KAFKA_CONNECTION_TIMEOUT_MS=3000

# Request timeout for Kafka operations
# Already defined above: KAFKA_REQUEST_TIMEOUT_MS=5000

# Kafka Topics Configuration
# Individual topic names (can be overridden for different environments)
KAFKA_TOPIC_AGENT_ROUTING=agent-routing-decisions
KAFKA_TOPIC_AGENT_ACTIONS=agent-actions
KAFKA_TOPIC_AGENT_TRANSFORMATIONS=agent-transformation-events
KAFKA_TOPIC_ROUTER_PERFORMANCE=router-performance-metrics

# Event Preload Configuration
# Set to 'false' to disable preloading historical data from PostgreSQL on startup
# When enabled, dashboards populate immediately with last 200 events
ENABLE_EVENT_PRELOAD=true

# =====================================================
# EVENT CONSUMER CONFIGURATION
# =====================================================
# Data Retention Settings
# How long to keep events in memory before pruning (milliseconds)
# Default: 86400000 (24 hours)
EVENT_DATA_RETENTION_MS=86400000

# How often to prune old data from memory (milliseconds)
# Default: 3600000 (1 hour)
EVENT_PRUNE_INTERVAL_MS=3600000

# Buffer Size Configuration
# Maximum number of recent actions to keep in memory
EVENT_MAX_ACTIONS=100

# Maximum number of routing decisions to keep in memory
EVENT_MAX_DECISIONS=100

# Maximum number of transformation events to keep in memory
EVENT_MAX_TRANSFORMATIONS=100

# Maximum number of performance metrics to keep in memory
EVENT_MAX_PERFORMANCE_METRICS=200

# Consumer Retry Configuration
# Maximum number of retry attempts for Kafka connection
KAFKA_MAX_RETRIES=5

# Base delay for exponential backoff (milliseconds)
# Test environment: 20ms, Production: 1000ms
KAFKA_RETRY_BASE_DELAY_MS=1000

# Maximum delay between retries (milliseconds)
# Test environment: 200ms, Production: 30000ms (30 seconds)
KAFKA_RETRY_MAX_DELAY_MS=30000

# =====================================================
# WEBSOCKET CONFIGURATION
# =====================================================
# WebSocket Server Settings
# Path for WebSocket connections
WS_PATH=/ws

# Heartbeat interval for client connection monitoring (milliseconds)
# Default: 30000 (30 seconds)
WS_HEARTBEAT_INTERVAL_MS=30000

# Maximum number of missed pings before terminating connection
# Default: 2 (allows 60 seconds total before disconnect)
WS_MAX_MISSED_PINGS=2

# WebSocket reconnection settings (client-side)
# Initial reconnection delay (milliseconds)
WS_RECONNECT_DELAY_MS=1000

# Maximum reconnection delay (milliseconds)
WS_RECONNECT_MAX_DELAY_MS=30000

# Maximum number of reconnection attempts
WS_RECONNECT_MAX_ATTEMPTS=10

# =====================================================
# SERVICE HEALTH CHECK CONFIGURATION
# =====================================================
# Health Check Timeouts
# Timeout for service health checks (milliseconds)
HEALTH_CHECK_TIMEOUT_MS=3000

# Connection timeout for health check requests (milliseconds)
HEALTH_CHECK_CONNECTION_TIMEOUT_MS=3000

# Cache TTL for health check results (milliseconds)
# Default: 60000 (1 minute)
HEALTH_CHECK_CACHE_TTL_MS=60000

# Health check interval for periodic checks (milliseconds)
# Default: 300000 (5 minutes)
HEALTH_CHECK_INTERVAL_MS=300000

# =====================================================
# CLIENT-SIDE SERVICE ENDPOINTS (VITE_)
# =====================================================
# These are exposed to the browser client via Vite's import.meta.env
# Must be prefixed with VITE_ to be accessible in client code

# Intelligence Service URL for client-side API calls
# Used by code intelligence, knowledge graph, and platform health dashboards
# Default: http://localhost:8053 (same as server-side INTELLIGENCE_SERVICE_URL)
VITE_INTELLIGENCE_SERVICE_URL=http://localhost:8053

# API Base URL for client-side API calls
# Default: Uses relative URLs (recommended for same-origin deployments)
# VITE_API_BASE_URL=http://localhost:3000

# =====================================================
# TESTING & DEVELOPMENT CONFIGURATION
# =====================================================
# Test Environment Detection
# Automatically set by test runners (vitest, jest)
# VITEST=true
# NODE_ENV=test

# Test API Configuration
# Base URL for API tests (defaults to http://localhost:3000)
TEST_BASE_URL=http://localhost:3000

# Test timeout for async operations (milliseconds)
TEST_TIMEOUT_MS=5000

# Test database configuration (if using separate test database)
# TEST_DATABASE_URL=postgresql://postgres:password@localhost:5436/omnidash_test

# Test Kafka configuration (if using separate test broker)
# TEST_KAFKA_BROKERS=localhost:9092

# =====================================================
# OBSERVABILITY & MONITORING
# =====================================================
# Metric Collection Interval
# How often to collect and aggregate metrics (milliseconds)
# Default: 60000 (1 minute)
METRICS_COLLECTION_INTERVAL_MS=60000

# Metric Retention
# How long to keep aggregated metrics (milliseconds)
# Default: 2592000000 (30 days)
METRICS_RETENTION_MS=2592000000

# Request Logging Configuration
# Log level for request logging (debug, info, warn, error)
REQUEST_LOG_LEVEL=info

# Maximum response body length to log (characters)
REQUEST_LOG_MAX_BODY_LENGTH=80

# Enable detailed event consumer logging
EVENT_CONSUMER_VERBOSE_LOGGING=false

# =====================================================
# PERFORMANCE TUNING
# =====================================================
# Database Connection Pool
# Maximum number of database connections in pool
DB_POOL_MAX_CONNECTIONS=20

# Minimum number of database connections in pool
DB_POOL_MIN_CONNECTIONS=2

# Connection idle timeout (milliseconds)
DB_POOL_IDLE_TIMEOUT_MS=30000

# API Rate Limiting
# Maximum requests per minute per client
API_RATE_LIMIT_PER_MINUTE=100

# Burst allowance for rate limiting
API_RATE_LIMIT_BURST=20

# Response Caching
# Enable response caching for GET requests
ENABLE_RESPONSE_CACHE=true

# Cache TTL for API responses (seconds)
API_CACHE_TTL_SECONDS=60
