# Configuration file for Omnidash
# Copy this file to .env and configure based on your deployment needs

# =====================================================
# POSTGRESQL DATABASE CONFIGURATION (REQUIRED)
# =====================================================
# Omnidash requires a PostgreSQL database for agent observability and intelligence tracking.
# This connects to the omninode-bridge PostgreSQL instance on 192.168.86.200:5436.
#
# Database: omninode_bridge
# Tables: 30+ tables for agent routing, workflow execution, pattern learning, LLM calls, etc.
# See: shared/intelligence-schema.ts for complete schema
#
# Connection Method 1: Individual variables (used by server/storage.ts)
POSTGRES_HOST=192.168.86.200
POSTGRES_PORT=5436
POSTGRES_DATABASE=omninode_bridge
POSTGRES_USER=postgres
POSTGRES_PASSWORD=YOUR_DATABASE_PASSWORD_HERE

# Connection Method 2: Connection string (takes priority if set)
# DATABASE_URL=postgresql://postgres:YOUR_DATABASE_PASSWORD_HERE@192.168.86.200:5436/omninode_bridge

# =====================================================
# LOCAL HOSTING CONFIGURATION
# =====================================================
# Service Ports Configuration
# These ports are used for external access to the services
HOST=localhost
ARCHON_SERVER_PORT=8181
ARCHON_MCP_PORT=8051
ARCHON_AGENTS_PORT=8052
ARCHON_UI_PORT=3737
ARCHON_DOCS_PORT=3838
INTELLIGENCE_SERVICE_PORT=8053
BRIDGE_SERVICE_PORT=8054
SEARCH_SERVICE_PORT=8055

# Service URLs Configuration (derived from HOST and ports above)
ARCHON_SERVER_URL=http://localhost:8181
ARCHON_MCP_URL=http://localhost:8051
ARCHON_AGENTS_URL=http://localhost:8052
INTELLIGENCE_SERVICE_URL=http://localhost:8053
BRIDGE_SERVICE_URL=http://192.168.86.200:8054
SEARCH_SERVICE_URL=http://localhost:8055

# =====================================================
# OLLAMA CONFIGURATION
# =====================================================
# Your existing Ollama server configuration
# The system will use this URL for Ollama API calls
# This will be set in the Settings UI after startup, but documented here for reference
# LLM_BASE_URL=http://192.168.86.200:11434/v1

# =====================================================
# VLLM AI PC CONFIGURATION (192.168.86.201)
# =====================================================
# High-performance vLLM endpoints on AI PC (2-5x faster than Ollama)
# DeepSeek-Coder-V2-Lite-Instruct for code generation
VLLM_DEEPSEEK_URL=http://192.168.86.201:8000/v1

# Meta-Llama-3.1-8B-Instruct for test generation and documentation
VLLM_LLAMA_URL=http://192.168.86.201:8001/v1

# =====================================================
# MCP TIMEOUT CONFIGURATION
# =====================================================
# Timeout settings for MCP service HTTP requests
MCP_REQUEST_TIMEOUT=60.0
MCP_CONNECT_TIMEOUT=10.0
MCP_READ_TIMEOUT=45.0
MCP_WRITE_TIMEOUT=15.0

# Bridge sync polling configuration
MCP_POLLING_TIMEOUT=60.0
MCP_POLLING_READ_TIMEOUT=30.0
MCP_MAX_POLLING_ATTEMPTS=30
MCP_POLLING_BASE_INTERVAL=1.0
MCP_POLLING_MAX_INTERVAL=5.0

# =====================================================
# LOGGING & DEBUGGING (OPTIONAL)
# =====================================================
# Set log level for debugging
LOGFIRE_TOKEN=
LOG_LEVEL=INFO

# AI Model API Keys
# Google Gemini API key for cloud-based inference (copied from omnibase_3)
GOOGLE_API_KEY=YOUR_GOOGLE_API_KEY_HERE
GEMINI_API_KEY=YOUR_GEMINI_API_KEY_HERE

# Z.AI API key for GLM-4.6 inference (add your key here after subscribing)
Z_AI_API_KEY=YOUR_Z_AI_API_KEY_HERE
Z_AI_API_URL=https://api.z.ai/api/coding/paas/v4

# Embedding Configuration
# Dimensions for embedding vectors (768 for nomic-embed-text, 1024 for mxbai-embed-large)
# Set to 768 since nomic-embed-text is recommended for Ollama
EMBEDDING_DIMENSIONS=768

# Service authentication token for internal API calls
SERVICE_AUTH_TOKEN=YOUR_SERVICE_AUTH_TOKEN_HERE

# OpenAI API Key for embeddings (currently using dummy - switch to Ollama or add real key)
OPENAI_API_KEY=sk-dummy-key-replace-with-real-or-use-ollama

# =====================================================
# AGENT REGISTRY CONFIGURATION
# =====================================================
# Path to agent definitions directory (omniclaude repository)
# If not set, defaults to ../omniclaude/agents/definitions (sibling directory)
# AGENT_DEFINITIONS_PATH=/path/to/omniclaude/agents/definitions

# =====================================================
# DEVELOPMENT & TESTING CONFIGURATION
# =====================================================
# Mock Data System - Controls all data fetching behavior
# When enabled, ALL data sources return mock data (no API/DB/Kafka calls)
#
# Set to 'true' to enable mock-only mode:
# - No API calls to backend services
# - No PostgreSQL database connections
# - No Kafka/Redpanda event streaming
# - Instant responses with realistic mock data
#
# Use cases:
# - Development when backend services are unavailable
# - Testing with consistent, predictable data
# - Demos and screenshots
# - Offline development
#
# Default: false (use real API endpoints)
# See: DATA_SOURCES.md for complete documentation
VITE_USE_MOCK_DATA=false

# =====================================================
# VALKEY CACHE SECURITY CONFIGURATION
# =====================================================
# Password for Valkey (Redis fork) distributed cache
# IMPORTANT: Change this in production deployments!
VALKEY_PASSWORD=YOUR_VALKEY_PASSWORD_HERE
VALKEY_URL=redis://:YOUR_VALKEY_PASSWORD_HERE@archon-valkey:6379/0
GH_PAT=YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE

# =====================================================
# EXTERNAL MCP GATEWAY CONFIGURATION
# =====================================================
# Enable external MCP service integration (zen, codanna, serena, etc.)
ARCHON_ENABLE_EXTERNAL_GATEWAY=true

# =====================================================
# KAFKA/REDPANDA CONFIGURATION
# =====================================================
# Remote Redpanda broker on 192.168.86.200
#
# Port Selection:
#   - Port 29092: External published port for host applications (RECOMMENDED for omnidash)
#   - Port 9092: Internal Docker network port (for containerized services only)
#
# Omnidash runs as a host process (not in Docker), so it uses port 29092.
# The server validates broker reachability at startup and logs connection status.

KAFKA_BROKERS=192.168.86.200:29092

# Legacy/compatibility (use KAFKA_BROKERS instead)
KAFKA_BOOTSTRAP_SERVERS=192.168.86.200:29092

# Kafka topics
KAFKA_DOC_TOPIC=dev.omniclaude.docs.evt.documentation-changed.v1

# Feature flags
ENABLE_EVENT_INTELLIGENCE=true
ENABLE_KAFKA_LOGGING=true
ENABLE_REAL_TIME_EVENTS=true

# Performance tuning (milliseconds)
KAFKA_REQUEST_TIMEOUT_MS=5000
KAFKA_PATTERN_DISCOVERY_TIMEOUT_MS=5000
KAFKA_CODE_ANALYSIS_TIMEOUT_MS=10000
KAFKA_QUALITY_ASSESSMENT_TIMEOUT_MS=10000

# =====================================================
# SERVER PORT CONFIGURATION
# =====================================================
# Port for the omnidash Express server (development and production)
PORT=3000
